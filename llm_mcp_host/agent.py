"""
ä¸»Agentæ¨¡å—
æ•´åˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼Œæä¾›å®Œæ•´çš„MCPè¯­éŸ³ä»£ç†åŠŸèƒ½
"""
import asyncio
import json
import traceback
from typing import List, Dict, Any, Optional

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionMessageParam, ChatCompletionToolParam
from config import config as global_config
from logger import logger

from .utils import parse_server_config, mcp_tools_to_openai_tools
from .context_manager import ContextManager
from .vector_tool_selector import VectorToolSelector as ToolSelector, ToolInfo
from .mcp_manager import MCPServerManager
from .prompt_manager import PromptManager


class MCPVoiceAgent:
    """å¢å¼ºçš„MCPè¯­éŸ³ä»£ç†ï¼Œæ”¯æŒå¤šæœåŠ¡å™¨ã€å‘é‡æœç´¢ã€ä¸Šä¸‹æ–‡ç®¡ç†ç­‰"""
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–MCPè¯­éŸ³ä»£ç†
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€é…ç½®
        """
        self.config = config or global_config
        
        # è§£æé…ç½®
        self.llm_config = self.config.llm
        self.system_prompt = self.config.system_prompt
        
        # å·¥å…·é€‰æ‹©é…ç½®
        self.tool_selection_config = self.config.get('tool_selection', {})
        self.top_k = self.tool_selection_config.get('top_k', 3)
        
        # ä¸Šä¸‹æ–‡é…ç½®
        self.context_config = self.config.get('context', {})
        self.max_turns = self.context_config.get('max_turns', 3)
        self.max_time_minutes = self.context_config.get('max_time_minutes', 30)
        self.enable_summarization = self.context_config.get('enable_summarization', False)
        self.summary_role = self.context_config.get('summary_role', 'user')

        # æ€»ç»“é…ç½®
        self.summarization_config = self.context_config.get('summarization', {})
        self.max_summary_tokens = self.summarization_config.get('max_summary_tokens', 200)
        self.summary_prompt = self.summarization_config.get('summary_prompt',
            "è¯·ç”¨ä¸­æ–‡ç®€æ´æ€»ç»“ä»¥ä¸‹å¯¹è¯å†å²ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼Œæ€»ç»“é•¿åº¦ä¸è¶…è¿‡{max_tokens}ä¸ªtokenï¼š")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.openai_client = AsyncOpenAI(
            api_key=self.llm_config["api_key"],
            base_url=self.llm_config.get("base_url")
        )
        
        self.context_manager = ContextManager(
            max_turns=self.max_turns,
            max_time_seconds=self.max_time_minutes * 60,
            system_prompt=self.system_prompt,
            enable_summarization=self.enable_summarization,
            summary_role=self.summary_role,
            max_summary_tokens=self.max_summary_tokens,
            summary_prompt=self.summary_prompt,
            openai_client=self.openai_client  # ä¼ é€’LLMå®¢æˆ·ç«¯ç”¨äºç”Ÿæˆæ€»ç»“
        )
        
        self.tool_selector = ToolSelector(top_k=self.top_k, config=self.tool_selection_config)
        self.mcp_manager = MCPServerManager()
        self.prompt_manager = PromptManager(system_prompt=self.system_prompt)
        
        # å·¥å…·åˆ—è¡¨ï¼ˆOpenAIæ ¼å¼ï¼‰
        self.openai_tools: List[ChatCompletionToolParam] = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_turns": 0,
            "total_tool_calls": 0,
            "total_errors": 0,
        }
    
    async def __aenter__(self):
        """åˆå§‹åŒ–è¿æ¥"""
        logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºç‰ˆMCPè¯­éŸ³ä»£ç†...")
        
        # è§£ææœåŠ¡å™¨é…ç½®
        server_configs = parse_server_config({
            "mcp_server": self.config.mcp_server,
            "mcp_servers": self.config.get("mcp_servers", {})
        })
        
        # è¿æ¥æ‰€æœ‰æœåŠ¡å™¨
        connection_tasks = []
        for server_name, server_config in server_configs.items():
            task = self.mcp_manager.add_server(server_name, server_config)
            connection_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰è¿æ¥å®Œæˆ
        results = await asyncio.gather(*connection_tasks, return_exceptions=True)
        
        # ç»Ÿè®¡æˆåŠŸè¿æ¥çš„æœåŠ¡å™¨
        successful_connections = sum(1 for r in results if r is True)
        logger.info(f"âœ… æˆåŠŸè¿æ¥ {successful_connections}/{len(server_configs)} ä¸ªMCPæœåŠ¡å™¨")
        
        # è·å–æ‰€æœ‰å·¥å…·
        all_tools = await self.mcp_manager.get_all_tools()
        logger.info(f"ğŸ› ï¸  æ€»å…±åŠ è½½ {len(all_tools)} ä¸ªå·¥å…·")
        
        # æ„å»ºå·¥å…·ç´¢å¼•
        self.tool_selector.build_index(all_tools)
        
        # è½¬æ¢ä¸ºOpenAIå·¥å…·æ ¼å¼
        self._update_openai_tools(all_tools)
        
        # è·å–MCPæç¤º
        mcp_prompts = await self.mcp_manager.get_all_prompts()
        if mcp_prompts:
            self.prompt_manager.add_mcp_prompts(mcp_prompts)
            logger.info(f"ğŸ“ åŠ è½½ {len(mcp_prompts)} ä¸ªMCPæç¤º")
        
        # æ›´æ–°ç³»ç»Ÿæç¤ºä»¥åŒ…å«MCPä¸Šä¸‹æ–‡
        combined_prompt = self.prompt_manager.get_combined_prompt(include_mcp_context=True)
        self.context_manager.clear()
        self.context_manager.add_message(
            {"role": "system", "content": combined_prompt},
            is_system=True
        )
        
        logger.info("ğŸ‰ å¢å¼ºç‰ˆMCPè¯­éŸ³ä»£ç†å°±ç»ª")
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ”Œ å…³é—­å¢å¼ºç‰ˆMCPè¯­éŸ³ä»£ç†...")
        await self.mcp_manager.close()
        logger.info("ğŸ‘‹ å¢å¼ºç‰ˆMCPè¯­éŸ³ä»£ç†å·²å…³é—­")
    
    async def chat(self, user_text: str) -> str:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè¿”å›åŠ©æ‰‹å“åº”
        
        Args:
            user_text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            
        Returns:
            åŠ©æ‰‹å“åº”æ–‡æœ¬
        """
        if not user_text or not user_text.strip():
            return ""
        
        logger.info(f"ğŸ‘‚ å¬åˆ°: {user_text}")
        self.stats["total_turns"] += 1
        
        # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
        self.context_manager.add_message({"role": "user", "content": user_text})
        
        # å¤„ç†LLMå›åˆ
        final_response = await self._process_llm_turn(user_text)
        
        logger.info(f"ğŸ—£ï¸  å›å¤: {final_response}")
        return final_response
    
    async def _process_llm_turn(self, user_query: str) -> str:
        """å¤„ç†å•ä¸ªLLMå›åˆï¼Œæ”¯æŒå¤šæ­¥å·¥å…·è°ƒç”¨"""
        # æ ¹æ®ç”¨æˆ·æŸ¥è¯¢é€‰æ‹©æœ€ç›¸å…³çš„å·¥å…·
        relevant_tools = self.tool_selector.search(user_query)
        
        # å¦‚æœæ‰¾åˆ°ç›¸å…³å·¥å…·ï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™ä½¿ç”¨æ‰€æœ‰å·¥å…·
        tools_to_use = self.openai_tools
        if relevant_tools:
            # åªä½¿ç”¨ç›¸å…³å·¥å…·
            relevant_tool_names = [tool.name for tool in relevant_tools]
            tools_to_use = [
                tool for tool in self.openai_tools
                if tool["function"]["name"] in relevant_tool_names
            ]
            logger.info(f"ğŸ” é€‰æ‹©äº† {len(tools_to_use)} ä¸ªç›¸å…³å·¥å…·: {relevant_tool_names}")
        
        while True:
            # è·å–å½“å‰æ¶ˆæ¯
            messages = self.context_manager.get_messages()
            
            # è°ƒç”¨LLM
            response = await self.openai_client.chat.completions.create(
                model=self.llm_config["model"],
                messages=messages,
                tools=tools_to_use if tools_to_use else None,
                temperature=self.llm_config.get("temperature", 0.7),
            )
            
            response_message = response.choices[0].message
            self.context_manager.add_message(response_message.to_dict())
            
            # æƒ…å†µA: LLMå†³å®šè°ƒç”¨å·¥å…·
            if response_message.tool_calls:
                logger.info(
                    f"ğŸ¤– éœ€è¦æ‰§è¡ŒåŠ¨ä½œ: {[t.function.name for t in response_message.tool_calls]}"
                )
                self.stats["total_tool_calls"] += len(response_message.tool_calls)
                
                for tool_call in response_message.tool_calls:
                    tool_name = tool_call.function.name
                    try:
                        fn_args = json.loads(tool_call.function.arguments)
                    except json.JSONDecodeError as e:
                        content_str = f"å‚æ•°è§£æé”™è¯¯: {str(e)}"
                        self.stats["total_errors"] += 1
                    else:
                        try:
                            # æ‰§è¡ŒMCPå·¥å…·
                            result = await self.mcp_manager.call_tool(tool_name, fn_args)
                            
                            # è½¬æ¢å·¥å…·ç»“æœä¸ºå­—ç¬¦ä¸²
                            content_str = ""
                            if result.content:
                                for item in result.content:
                                    if item.type == "text":
                                        content_str += item.text
                                    else:
                                        content_str += str(item)
                            else:
                                content_str = "æˆåŠŸ"
                        except Exception as e:
                            content_str = f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
                            self.stats["total_errors"] += 1
                            logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥ {tool_name}: {e}")
                    
                    # å°†å·¥å…·ç»“æœè¿”å›ç»™LLM
                    self.context_manager.add_message({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": content_str,
                    })
                
                # å¾ªç¯ç»§ç»­ï¼ŒLLMå°†çœ‹åˆ°å·¥å…·ç»“æœå¹¶ç”Ÿæˆæ–°å“åº”
            
            # æƒ…å†µB: LLMç”Ÿæˆæœ€ç»ˆæ–‡æœ¬å“åº”
            else:
                return response_message.content or ""
    
    def _update_openai_tools(self, tools: List[ToolInfo]):
        """æ›´æ–°OpenAIå·¥å…·åˆ—è¡¨"""
        # åˆ›å»ºæ¨¡æ‹Ÿçš„MCPå·¥å…·ç»“æœ
        class MockTool:
            def __init__(self, name, description, inputSchema):
                self.name = name
                self.description = description
                self.inputSchema = inputSchema
        
        class MockToolsResult:
            def __init__(self, tools):
                self.tools = tools
        
        # æ„å»ºæ¨¡æ‹Ÿå·¥å…·åˆ—è¡¨
        mock_tools = []
        for tool in tools:
            mock_tool = MockTool(
                name=tool.name,
                description=tool.description,
                inputSchema=tool.parameters
            )
            mock_tools.append(mock_tool)
        
        # è½¬æ¢ä¸ºOpenAIæ ¼å¼
        mock_result = MockToolsResult(mock_tools)
        self.openai_tools = mcp_tools_to_openai_tools(mock_result)
    
    async def load_prompt(self, prompt_name: str, **kwargs) -> Optional[str]:
        """åŠ è½½MCPæç¤º"""
        return await self.prompt_manager.load_prompt(prompt_name, self.mcp_manager, **kwargs)
    
    def get_context_stats(self) -> Dict[str, Any]:
        """è·å–ä¸Šä¸‹æ–‡ç»Ÿè®¡ä¿¡æ¯"""
        return self.context_manager.get_stats()
    
    def get_tool_stats(self) -> Dict[str, Any]:
        """è·å–å·¥å…·ç»Ÿè®¡ä¿¡æ¯"""
        return self.tool_selector.get_stats()
    
    def get_mcp_stats(self) -> Dict[str, Any]:
        """è·å–MCPç»Ÿè®¡ä¿¡æ¯"""
        return self.mcp_manager.get_stats()
    
    def get_prompt_stats(self) -> Dict[str, Any]:
        """è·å–æç¤ºç»Ÿè®¡ä¿¡æ¯"""
        return self.prompt_manager.get_stats()
    
    def get_agent_stats(self) -> Dict[str, Any]:
        """è·å–ä»£ç†æ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "context": self.get_context_stats(),
            "tools": self.get_tool_stats(),
            "mcp": self.get_mcp_stats(),
            "prompts": self.get_prompt_stats(),
        }
    
    def clear_context(self):
        """æ¸…ç©ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        self.context_manager.clear()
        logger.info("ğŸ§¹ å¯¹è¯ä¸Šä¸‹æ–‡å·²æ¸…ç©º")


def process_llm_host(text_queue, tts_queue, interrupt_event):
    """
    MCP Hostä¸»è¿›ç¨‹å‡½æ•°
    ä¿æŒä¸åŸæœ‰æ¥å£å…¼å®¹
    
    Args:
        text_queue: æ–‡æœ¬è¾“å…¥é˜Ÿåˆ—ï¼ˆSTT -> LLMï¼‰
        tts_queue: æ–‡æœ¬è¾“å‡ºé˜Ÿåˆ—ï¼ˆLLM -> TTSï¼‰
        interrupt_event: ä¸­æ–­äº‹ä»¶
    """
    logger.info("[LLM] å¢å¼ºç‰ˆMCP Hostè¿›ç¨‹å¯åŠ¨...")
    
    async def voice_assistant_loop(text_queue, tts_queue, interrupt_event):
        async with MCPVoiceAgent() as agent:
            while not interrupt_event.is_set():
                try:
                    # 1. ä»STTè·å–æ–‡æœ¬
                    stt_text = text_queue.get()
                    if interrupt_event.is_set():
                        break
                    
                    # 2. è°ƒç”¨MCPä»£ç†
                    tts_text = await agent.chat(stt_text)
                    
                    # 3. å‘é€åˆ°TTS
                    tts_queue.put(tts_text)
                    
                    # è®°å½•ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯5è½®ï¼‰
                    if agent.stats["total_turns"] % 5 == 0:
                        stats = agent.get_agent_stats()
                        logger.info(f"ğŸ“Š ä»£ç†ç»Ÿè®¡: {stats}")
                    
                    logger.info("-" * 50)
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å¾ªç¯é”™è¯¯: {e}")
                    print(traceback.format_exc())
                    if interrupt_event.is_set():
                        break
    
    try:
        asyncio.run(voice_assistant_loop(text_queue, tts_queue, interrupt_event))
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢LLM Hostè¿›ç¨‹")
    except Exception as e:
        logger.error(f"âŒ LLM Hostè¿è¡Œæ—¶é”™è¯¯: {e}")
    finally:
        logger.info("[LLM] å¢å¼ºç‰ˆMCP Hostè¿›ç¨‹ç»“æŸ")

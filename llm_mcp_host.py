import asyncio
import json
import os
import sys
from typing import List, Dict, Any, Optional
from contextlib import asynccontextmanager, AsyncExitStack

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionMessageParam, ChatCompletionToolParam
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client

from config import config


# --- å·¥å…·è½¬æ¢é€»è¾‘ ---
def mcp_tools_to_openai_tools(mcp_list_tools_result) -> List[ChatCompletionToolParam]:
    openai_tools: List[ChatCompletionToolParam] = []
    for tool in mcp_list_tools_result.tools:
        openai_tools.append(
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema,
                },
            }
        )
    return openai_tools


# --- ä¼ è¾“å±‚å·¥å‚ ---
@asynccontextmanager
async def mcp_transport_factory(config_dict: Dict[str, Any]):
    server_type = config_dict.get("type", "stdio")
    if server_type == "stdio":
        server_params = StdioServerParameters(
            command=config_dict["command"],
            args=config_dict.get("args", []),
            env={**os.environ, **config_dict.get("env", {})},
        )
        async with stdio_client(server_params) as streams:
            yield streams
    elif server_type == "sse":
        url = config_dict["url"]
        headers = config_dict.get("headers", {})
        async with sse_client(url=url, headers=headers) as streams:
            yield streams
    else:
        raise ValueError(f"Unsupported MCP server type: {server_type}")


class MCPVoiceAgent:
    def __init__(self):
        self.mcp_config = config.mcp_server
        self.llm_config = config.llm

        # 1. é…ç½®ç³»ç»Ÿæç¤ºè¯ (System Prompt)
        self.system_prompt = config.system_prompt

        self.openai_client = AsyncOpenAI(
            api_key=self.llm_config["api_key"], base_url=self.llm_config.get("base_url")
        )

        # åˆå§‹åŒ–æ¶ˆæ¯å†å²
        self.messages: List[ChatCompletionMessageParam] = [
            {"role": "system", "content": self.system_prompt}
        ]

        self.session: Optional[ClientSession] = None
        self.openai_tools: List[ChatCompletionToolParam] = []
        self._exit_stack = AsyncExitStack()

    # --- ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šè´Ÿè´£è¿æ¥çš„å»ºç«‹ä¸ä¿æŒ ---
    async def __aenter__(self):
        """åˆå§‹åŒ– MCP è¿æ¥å’Œ Session"""
        print(f"ğŸ”Œ Connecting to MCP Server...")

        # ä½¿ç”¨ ExitStack ç®¡ç†åµŒå¥—çš„ä¸Šä¸‹æ–‡
        read, write = await self._exit_stack.enter_async_context(
            mcp_transport_factory(self.mcp_config)
        )

        self.session = await self._exit_stack.enter_async_context(
            ClientSession(read, write)
        )

        await self.session.initialize()

        # åŠ è½½å·¥å…·
        tools_result = await self.session.list_tools()
        self.openai_tools = mcp_tools_to_openai_tools(tools_result)
        print(f"ğŸ› ï¸  MCP Agent Ready. Loaded {len(self.openai_tools)} tools.")

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """æ¸…ç†èµ„æºï¼Œæ–­å¼€è¿æ¥"""
        print("ğŸ”Œ Disconnecting MCP Agent...")
        await self._exit_stack.aclose()

    # --- æ ¸å¿ƒåŠŸèƒ½ï¼šè¾“å…¥æ–‡æœ¬ -> æ‰§è¡Œæ“ä½œ -> è¾“å‡ºå›å¤æ–‡æœ¬ ---
    async def chat(self, user_text: str) -> str:
        """
        å¯¹åº”æµç¨‹å›¾ä¸­çš„ E -> F -> G çš„è¾“å…¥è¾“å‡ºæ¥å£
        Input: STT è½¬æ¢åçš„æ–‡æœ¬
        Output: å‘é€ç»™ TTS çš„æ–‡æœ¬
        """
        if not user_text or not user_text.strip():
            return ""

        print(f"\nğŸ‘‚ Hearing: {user_text}")
        self.messages.append({"role": "user", "content": user_text})

        # è¿›å…¥ LLM å¤„ç†å¾ªç¯ï¼ˆå¤„ç†å¯èƒ½çš„å¤šæ¬¡å·¥å…·è°ƒç”¨ï¼‰
        final_response_text = await self._process_llm_turn()

        print(f"ğŸ—£ï¸  Speaking: {final_response_text}")
        return final_response_text

    async def _process_llm_turn(self) -> str:
        """å¤„ç†å•è½®å¯¹è¯åŠå¤šæ­¥å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆç»™ç”¨æˆ·çš„æ–‡æœ¬"""
        while True:
            response = await self.openai_client.chat.completions.create(
                model=self.llm_config["model"],
                messages=self.messages,
                tools=self.openai_tools if self.openai_tools else None,
                temperature=self.llm_config.get("temperature", 0.7),
            )

            response_message = response.choices[0].message
            self.messages.append(response_message)

            # æƒ…å†µ A: LLM å†³å®šè°ƒç”¨å·¥å…·
            if response_message.tool_calls:
                print(
                    f"ğŸ¤– Action required: {[t.function.name for t in response_message.tool_calls]}"
                )

                for tool_call in response_message.tool_calls:
                    fn_name = tool_call.function.name
                    fn_args = json.loads(tool_call.function.arguments)

                    try:
                        # æ‰§è¡Œ MCP å·¥å…·
                        result = await self.session.call_tool(
                            fn_name, arguments=fn_args
                        )

                        # å°†å·¥å…·ç»“æœè½¬ä¸ºå­—ç¬¦ä¸²ä¾› LLM ç†è§£
                        content_str = ""
                        if result.content:
                            for item in result.content:
                                if item.type == "text":
                                    content_str += item.text
                                else:
                                    content_str += str(item)
                        else:
                            content_str = "Success"
                    except Exception as e:
                        content_str = f"Error executing tool: {str(e)}"

                    # å°†å·¥å…·ç»“æœå›ä¼ ç»™ LLM
                    self.messages.append(
                        {
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": content_str,
                        }
                    )
                # å¾ªç¯ç»§ç»­ï¼ŒLLM å°†çœ‹åˆ°å·¥å…·ç»“æœå¹¶ç”Ÿæˆæ–°çš„å›å¤

            # æƒ…å†µ B: LLM ç”Ÿæˆäº†æœ€ç»ˆæ–‡æœ¬å›å¤
            else:
                return response_message.content


def process_llm_host(text_queue, tts_queue, interrupt_event):
    """
    ä½œä¸º MCP Hostï¼Œæ¥æ”¶æ–‡æœ¬ï¼Œç®¡ç†ä¸Šä¸‹æ–‡ï¼Œè°ƒç”¨å·¥å…·ï¼Œå¹¶å°†ç”Ÿæˆçš„æ–‡æœ¬æµå¼ä¼ è¾“ç»™ TTSã€‚
    """
    print("[LLM] è¿›ç¨‹å¯åŠ¨...")

    async def voice_assistant_loop(text_queue, tts_queue, interrupt_event):
        # ä½¿ç”¨ config æ¨¡å—è·å–é…ç½®
        # ä½¿ç”¨ context manager ä¿æŒ MCP è¿æ¥
        async with MCPVoiceAgent() as agent:
            while True:
                # 1. (æµç¨‹ D->E) ä» STT è·å–æ–‡æœ¬
                stt_text = text_queue.get()

                # 2. (æµç¨‹ E->F) è°ƒç”¨ MCP Agent
                tts_text = await agent.chat(stt_text)

                # 3. (æµç¨‹ F->G) å‘é€ç»™ TTS
                tts_queue.put(tts_text)

                print("-" * 50)

    try:
        asyncio.run(voice_assistant_loop(text_queue, tts_queue, interrupt_event))
    except Exception as e:
        print(f"è¿è¡Œå‡ºé”™: {e}")
